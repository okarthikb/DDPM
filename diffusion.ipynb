{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRm5nlUUOgEA"
      },
      "source": [
        "### **Diffusion!**\n",
        "\n",
        "![](https://raw.githubusercontent.com/okarthikb/diffusion/main/diffusion.png)\n",
        "\n",
        "Unconditional DDPM training and sampling algorithm from [Ho et. al.](https://arxiv.org/abs/2006.11239) We implement the batched variants of these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4aNbklo61fr"
      },
      "outputs": [],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Lln4n96RJ1"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, matplotlib\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import imageio as iio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from torch import einsum\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms\n",
        "from einops import rearrange, repeat\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2 ** 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6xA7aZLlnSI"
      },
      "source": [
        "### U-Net architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3KoKMmd6RJ4"
      },
      "outputs": [],
      "source": [
        "def position_embeddings(l, d):\n",
        "  w = 1e-4 ** (repeat(torch.arange(2, d + 2, 2), 'l -> (l 2)') / d)\n",
        "  t = repeat(torch.arange(1, l + 1), 'l -> l d', d=d)\n",
        "  pos = w * t\n",
        "  pos[:, ::2], pos[:, 1::2] = torch.sin(pos[:, ::2]), torch.cos(pos[:, 1::2])\n",
        "  return pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCPTKmzm6RJ5"
      },
      "outputs": [],
      "source": [
        "# returns Conv2d (down) and ConvTranspose2d (up)\n",
        "# down and up will be s.t. x.shape = down(up(x)).shape = up(down(x)).shape\n",
        "def get_conv_down_up(\n",
        "  x_size, in_channels, out_channels, kernel_size, stride, padding\n",
        "):\n",
        "  down = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "  y_size = (x_size + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "  out_padding = x_size - ((y_size - 1) * stride - 2 * padding + kernel_size)\n",
        "  up = nn.ConvTranspose2d(\n",
        "    out_channels, in_channels, kernel_size, stride, padding, out_padding\n",
        "  )\n",
        "  return down, up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiNZN0e86RJ6"
      },
      "outputs": [],
      "source": [
        "# standard multi-head attention, d is channel dim instead of token emb dim here\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, n_head=1):  # self attention by default\n",
        "    super().__init__()\n",
        "    assert d % n_head == 0, 'n_head must divide d'\n",
        "    self.d, self.nh = d, n_head\n",
        "    self.wx, self.wo = nn.Linear(d, 3 * d), nn.Linear(d, d)\n",
        "    self.gn = nn.GroupNorm(1, d)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.wx(rearrange(self.gn(x), 'b c h w -> b (h w) c'))\n",
        "    q, k, v = rearrange(y, 'b n (t nh dh) -> t b nh n dh', t=3, nh=self.nh)\n",
        "    hs = F.softmax(einsum('bhic, bhjc -> bhij', q, k) / self.d ** 0.5, -1) @ v\n",
        "    att = self.wo(rearrange(hs, 'b nh l dh -> b l (nh dh)'))\n",
        "    return x + rearrange(att, 'b (h w) c -> b c h w', h=x.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SArFLk046RJ7"
      },
      "outputs": [],
      "source": [
        "# ConvNeXt from arXiv:2201.03545\n",
        "# we add time embedding to first conv layer output\n",
        "class ConvNeXtBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, t_emb_d, m=2):\n",
        "    super().__init__()\n",
        "    self.t_fc = nn.Linear(t_emb_d, in_channels) if t_emb_d else None\n",
        "    self.ds_conv = nn.Conv2d(in_channels, in_channels, 7, 1, 3, 1, in_channels)\n",
        "    self.sequential = nn.Sequential(\n",
        "      nn.GroupNorm(1, in_channels),\n",
        "      nn.Conv2d(in_channels, out_channels * m, 3, 1, 1),\n",
        "      nn.GELU(),\n",
        "      nn.GroupNorm(1, out_channels * m),\n",
        "      nn.Conv2d(out_channels * m, out_channels, 3, 1, 1)\n",
        "    )\n",
        "\n",
        "    if in_channels == out_channels:\n",
        "      self.shortcut = nn.Identity()\n",
        "    else:\n",
        "      self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "\n",
        "  def forward(self, x, t_emb=None):\n",
        "    x_proj = self.shortcut(x)\n",
        "\n",
        "    if t_emb is None:\n",
        "      return self.sequential(self.ds_conv(x)) + x_proj\n",
        "    else:\n",
        "      t_emb = rearrange((self.t_fc(F.silu(t_emb))), 'b c -> b c 1 1')\n",
        "      return self.sequential(self.ds_conv(x) + t_emb) + x_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw1yK_OmuYZo"
      },
      "source": [
        "![](https://raw.githubusercontent.com/okarthikb/diffusion/main/unet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOsf_5jl6RJ8"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, max_t, m, n_head):\n",
        "    super().__init__()\n",
        "    self.max_t, self.m = max_t, m\n",
        "\n",
        "    x_size = 28  # image size\n",
        "    t_emb_d = 4 * x_size  # time embedding dim = 4 x image size\n",
        "    self.t_emb = nn.Parameter(\n",
        "      position_embeddings(max_t + 1, t_emb_d), requires_grad=False\n",
        "    )\n",
        "\n",
        "    downscales, upscales = zip(\n",
        "      get_conv_down_up(x_size, 16, 32, 3, 2, 1),\n",
        "      get_conv_down_up(x_size, 32, 64, 3, 2, 1),\n",
        "      get_conv_down_up(x_size, 64, 128, 3, 3, 1)\n",
        "    )\n",
        "\n",
        "    self.downscales = nn.ModuleList(downscales)\n",
        "    self.upscales = nn.ModuleList(reversed(upscales))\n",
        "\n",
        "    Block = lambda in_channels, out_channels: nn.ModuleList([\n",
        "      ConvNeXtBlock(in_channels, out_channels, t_emb_d, m),\n",
        "      MultiHeadAttention(out_channels, n_head)\n",
        "    ])\n",
        "\n",
        "    self.downblocks = nn.ModuleList([\n",
        "      Block(1, 16), Block(32, 32), Block(64, 64), Block(128, 128)\n",
        "    ])\n",
        "    self.upblocks = nn.ModuleList([\n",
        "      Block(128, 64), Block(64, 32), Block(32, 8)\n",
        "    ])\n",
        "\n",
        "    self.final_conv = ConvNeXtBlock(8, 1, t_emb_d, m)\n",
        "\n",
        "  def forward(self, x, t=None):\n",
        "    cache = []\n",
        "\n",
        "    conv, attn = self.downblocks[0]\n",
        "    t_emb = None if t is None else self.t_emb.index_select(0, t)\n",
        "\n",
        "    x = attn(conv(x, t_emb))\n",
        "    for down, (conv, attn) in zip(self.downscales, self.downblocks[1:]):\n",
        "      cache.append(x)\n",
        "      x = attn(conv(down(x), t_emb))\n",
        "\n",
        "    for up, (conv, attn) in zip(self.upscales, self.upblocks):\n",
        "      x = attn(conv(torch.cat([up(x), cache.pop()], 1), t_emb))\n",
        "\n",
        "    return self.final_conv(x, t_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXnpi8fQp_YA"
      },
      "source": [
        "### Get forward/backward diffusion sampling functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n084D2QM6RJ9"
      },
      "outputs": [],
      "source": [
        "# function to get forward/reverse diffusion process samplers\n",
        "def get_samplers(max_t, device, s=0.008, shape=(1, 28, 28)):\n",
        "  ts = torch.arange(0, max_t + 1, device=device)\n",
        "\n",
        "  abar = ((torch.pi / 2) * (ts / max_t + s) / (1 + s)).cos() ** 2\n",
        "  abar = abar / abar[0]\n",
        "\n",
        "  beta = (1 - abar[1:] / abar[:-1]).clip(1e-4, 1 - 1e-4)\n",
        "  beta = torch.cat([torch.zeros(1, device=device), beta])\n",
        "  alpha, sigma = 1 - beta, beta.sqrt()\n",
        "\n",
        "  sqrt_abar, sqrt_1m_abar = abar.sqrt(), (1 - abar).sqrt()\n",
        "\n",
        "  coeff1, coeff2 = beta / sqrt_1m_abar, 1 / alpha.sqrt()\n",
        "\n",
        "  # reshape so we can multiply with image batch of shape (b c h w)\n",
        "  sqrt_abar = rearrange(sqrt_abar, 'b -> b 1 1 1')\n",
        "  sqrt_1m_abar = rearrange(sqrt_1m_abar, 'b -> b 1 1 1')\n",
        "\n",
        "  # forward process sampling at one t step\n",
        "  def q_sample(x, t):\n",
        "    eps = torch.randn_like(x, device=x.device)\n",
        "    mu_t = sqrt_abar.index_select(0, t) * x\n",
        "    std_t = sqrt_1m_abar.index_select(0, t)\n",
        "    return mu_t + std_t * eps, eps\n",
        "\n",
        "  # reverse process sampling all t steps (a trajectory, starting at z ~ N(0, I))\n",
        "  def p_sample_trajectory(model, n_sample=1, return_all_steps=False):\n",
        "    model.eval()\n",
        "\n",
        "    x_t = torch.randn(n_sample, *shape, device=device)\n",
        "    x_all = [x_t] if all else None\n",
        "    tb = repeat(ts, 't -> t b', b=n_sample)\n",
        "\n",
        "    # DDPM sampling algorithm (Algorithm 1)\n",
        "    with torch.no_grad():\n",
        "      for t in range(max_t, 0, -1):\n",
        "        z = torch.randn_like(x_t, device=device) if t > 1 else 0\n",
        "        eps = model(x_t, tb[t])\n",
        "        x_t = coeff2[t] * (x_t - coeff1[t] * eps) + sigma[t] * z\n",
        "        if all:\n",
        "          x_all.append(x_t)\n",
        "\n",
        "    return (x_t, x_all) if return_all_steps else x_t\n",
        "\n",
        "  return q_sample, p_sample_trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Fo9kMwkzeE"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvfyNFEP8EOs"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "batch_size = 128\n",
        "lr = 3e-4\n",
        "max_t = 500\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wf__Pwy6RJ-"
      },
      "outputs": [],
      "source": [
        "dataset = MNIST(\n",
        "  root='./',\n",
        "  download=True,\n",
        "  transform=transforms.Compose([\n",
        "    transforms.ToTensor(), transforms.Lambda(lambda x: 2 * x - 1)\n",
        "  ])\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "  dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WQsRXnoLw0n"
      },
      "outputs": [],
      "source": [
        "model = UNet(max_t=max_t, m=4, n_head=1).to(device)\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRubRaaEo4dW"
      },
      "outputs": [],
      "source": [
        "# we use q_sample below to return a batch of images, each image sampled from\n",
        "# a different timestep in the forward diffusion process\n",
        "q_sample, p_sample_trajectory = get_samplers(max_t, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e08UrDWE6RJ-"
      },
      "outputs": [],
      "source": [
        "# DDPM training algorithm (Algorithm 2)\n",
        "model.train()\n",
        "# epochs = 5  (train few epochs at a time, see if model is producing anything)\n",
        "for epoch in range(1, epochs + 1):\n",
        "  bar = tqdm(dataloader, ascii=' >=')\n",
        "  for x, _ in bar:\n",
        "    t = torch.randint(1, max_t + 1, (batch_size,), device=device)\n",
        "    x_t, eps = q_sample(x.to(device), t)\n",
        "    loss = F.mse_loss(model(x_t, t), eps)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52m4UtTGIM5"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Rerun cells to sample anew."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk3zWotyZkhA"
      },
      "outputs": [],
      "source": [
        "n_sample, rows = 25, 5\n",
        "reshape = lambda x: rearrange(x, '(r f) 1 h w -> (r h) (f w)', r=rows)\n",
        "\n",
        "x_0, x_all = p_sample_trajectory(model, n_sample, True)\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plt.imshow(reshape(x_0.cpu().numpy()))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn2phJ3kdCb_"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "\n",
        "skip = 2\n",
        "for x in x_all[max_t % skip::skip]:\n",
        "  images.append(reshape(x.cpu().numpy()))\n",
        "\n",
        "\n",
        "def update(i):\n",
        "  plt.clf()\n",
        "  plt.imshow(images[i])\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "plt.ioff()\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ani = animation.FuncAnimation(\n",
        "  fig, update, frames=range(len(images)), interval=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIWuYJlPQ-3G"
      },
      "outputs": [],
      "source": [
        "display(HTML(ani.to_jshtml()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}