{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w0vjOBuzE3gi"
      },
      "source": [
        "### **Diffusion!**\n",
        "\n",
        "![](https://raw.githubusercontent.com/okarthikb/diffusion/main/diffusion.png)\n",
        "\n",
        "Unconditional DDPM training and sampling algorithm from Ho et al. We implement the batched and conditional variant of these algorithms.\n",
        "\n",
        "Relevant papers, blogs, and videos (first 8 important):\n",
        "\n",
        "1. [Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function](https://arxiv.org/abs/1907.08956)\n",
        "2. [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970) (detailed paper, goes through the entire derivation; discusess SNR and score-based interpretation of diffusion models, along with classifier-free guidance)\n",
        "3. [Denoising diffusion probabilistic models from first principles](https://liorsinai.github.io/coding/2022/12/03/denoising-diffusion-1-spiral.html#reverse-process) (blog, ignore the Julia code, look @ the full derivation in the linked section)\n",
        "4. [Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications](https://www.youtube.com/watch?v=cS6JQpEY9cs&t=2010s) (video, pretty long, just most of the contents above in video form)\n",
        "5. [Proof: Kullback-Leibler divergence for the multivariate normal distribution](https://statproofbook.github.io/P/mvn-kl.html) (important proof, required to get the closed-form expression for reverse diffusion objective)\n",
        "6. [DDPMs](https://arxiv.org/abs/2006.11239) (the Ho et al. paper)\n",
        "7. [Improving DDPMs](https://arxiv.org/abs/2102.09672) (introduces cosine schedule and learnable variances for reverse diffusion)\n",
        "8. [DDIMs](https://arxiv.org/abs/2010.02502) (nice trick to speed up diffusion inference)\n",
        "9. [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/) (blog by Yang Song, a good primer on score functions and score-based generative modelling)\n",
        "10. [MIT 6.S192 - Lecture 22: Diffusion Probabilistic Models, Jascha Sohl-Dickstein](https://www.youtube.com/watch?v=XCUlnHP1TNM&t=1016s) (original diffusion paper author lecture)\n",
        "11. [Grokking Diffusion Models](https://nonint.com/2022/10/31/grokking-diffusion-models/) (this is re various ways to look @ diffusion models)\n",
        "12. [Iterative 𝛼-(de)Blending: a Minimalist Deterministic Diffusion Model](https://ggx-research.github.io/publication/2023/05/10/publication-iadb.html) (speaking of ways to look @ diffusion models, this one is particularly interesting)\n",
        "13. [On the Theory of Stochastic Processes, with Particular Reference to Applications](https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-First-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/On-the-Theory-of-Stochastic-Processes-with-Particular-Reference-to/bsmsp/1166219215.pdf) (old 1949 paper, relevant to equivalence of forward and reverse diffusion functional forms)\n",
        "14. [What are Diffusion Models?](https://www.youtube.com/watch?v=fbLgFrlTnGU&t=251s) (the video referencing the paper above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4aNbklo61fr",
        "outputId": "f4700942-2ae0-44cf-aae3-f93d5ce8fefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m1Lln4n96RJ1"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, matplotlib\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from torch import einsum\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms\n",
        "from einops import rearrange, repeat\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0jqFxYRRyXpY"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GpVoa27zFLuy"
      },
      "source": [
        "![](https://raw.githubusercontent.com/okarthikb/diffusion/main/U-Net.png)\n",
        "\n",
        "What we do with the U-Net:\n",
        "\n",
        "1. The input is a batch of images `x` of shape `(batch_size, 1, 28, 28)`, timesteps `t` of shape `(batch_size,)` (each image in the batch is from a different timestep in the forward diffusion process), and classes `classes` of shape `(batch_size,)` (this is for class conditioned generation; here it's a tensor of integers from 0 to 9, i.e., 10 classes).\n",
        "2. The output is also a batch of the same shape `(batch_size, 1, 28, 28)` as the input (see diagram above), and it's simply the model's predicted noise values for all pixels.\n",
        "3. We generate position embeddings of shape using the function defined below - `position_embeddings(l, d)` - which takes in `l` (which is the sequence length in language modelling whereas here it's the number of timesteps) and `d` (which is is the dimension the embedding).\n",
        "4. Set the time and class embedding dimension to 4 x image width/height (assuming a square image here). For inputs `t` and `classes` both of shape `(batch_size,)`, the embedding input will be of shape `(batch_size, 4 * x_size)`.\n",
        "5. The time embedding layer is not learned. It's prespecified and is of shape `(T, t_emb_d=4 * x_size)`. During training, we choose `batch_size` timesteps at random from 1 to T, then get the embedding for each timestep to make the time embedding input of shape `(batch_size, t_emb_d)`. This is what tells the model the forward diffusion timesteps for which it needs to predict noise values.\n",
        "6. The class embedding _is_ learned. We have 10 classes (digits from 0 to 9), so we create an embedding layer of shape `(n_class, class_emb_d)`. During training, for `batch_size` images, we receive `batch_size` classes (the label for each image) and we get the associated embedding for each class and form the class embedding input of shape `(batch_size, class_emb_d)`. Both `class_emb_d` and `t_emb_d` are the same.\n",
        "7. Each `Conv2d` or `ConvTranspose2d` layer has a specific number of input and output channels (an RGB image as 3 channels). We project the time and class embeddings from their dimension to the channel dimension (i.e., `in_channels` or the number of input channels) and add the same projection along the channel dimension at every height-width index.\n",
        "8. After each convolution block, we get an output of shape `(batch_size, n_channel, height, width)`. We reduce it to `(batch_size, n_channel, height * width)`, do self attention, and reshape to original shape.\n",
        "\n",
        "If you don't know what position embeddings are, check out [this](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) very neat post on the topic.\n",
        "\n",
        "If you're not upto speed on einsum/einops notation, check out [this](https://rockt.github.io/2018/04/30/einsum) post on einsum by Tim Rocktäschel and [this](https://einops.rocks/1-einops-basics/) tutorial on einops. [Some examples](https://einops.rocks/pytorch-examples.html) showing why einops is very convenient.\n",
        "\n",
        "You don't have to read the linked sources for this tutorial; it suffices to know what the input and output of the U-Net are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K3KoKMmd6RJ4"
      },
      "outputs": [],
      "source": [
        "def position_embeddings(l, d):\n",
        "  w = 1e-4 ** (repeat(torch.arange(2, d + 2, 2), 'l -> (l 2)') / d)\n",
        "  t = repeat(torch.arange(1, l + 1), 'l -> l d', d=d)\n",
        "  pos = w * t\n",
        "  pos[:, ::2], pos[:, 1::2] = torch.sin(pos[:, ::2]), torch.cos(pos[:, 1::2])\n",
        "  return pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kCPTKmzm6RJ5"
      },
      "outputs": [],
      "source": [
        "def conv_down_up(\n",
        "  x_size, in_channels, out_channels, kernel_size, stride, padding\n",
        "):\n",
        "  down = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "  y_size = (x_size + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "  out_padding = x_size - ((y_size - 1) * stride - 2 * padding + kernel_size)\n",
        "  up = nn.ConvTranspose2d(\n",
        "    out_channels, in_channels, kernel_size, stride, padding, out_padding\n",
        "  )\n",
        "  return down, up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WiNZN0e86RJ6"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d):\n",
        "    super().__init__()\n",
        "    self.scale = d ** -0.5\n",
        "    self.wx, self.wo = nn.Linear(d, 3 * d), nn.Linear(d, d)\n",
        "    self.gn = nn.GroupNorm(1, d)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.wx(rearrange(self.gn(x), 'b c h w -> b (h w) c'))\n",
        "    q, k, v = rearrange(z, 'b n (o d) -> o b n d', o=3)\n",
        "    A = self.wo(F.softmax(einsum('bic, bjc -> bij', q, k) * self.scale, -1) @ v)\n",
        "    return x + rearrange(A, 'b (h w) c -> b c h w', h=x.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SArFLk046RJ7"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, t_emb_d, class_emb_d, m=4):\n",
        "    super().__init__()\n",
        "    self.t_fc = nn.Linear(t_emb_d, in_channels) if t_emb_d else None\n",
        "    self.c_fc = nn.Linear(class_emb_d, in_channels) if class_emb_d else None\n",
        "\n",
        "    self.ds_conv = nn.Conv2d(in_channels, in_channels, 7, 1, 3, 1, in_channels)\n",
        "    self.sequential = nn.Sequential(\n",
        "      nn.GroupNorm(1, in_channels),\n",
        "      nn.Conv2d(in_channels, out_channels * m, 3, 1, 1),\n",
        "      nn.GELU(),\n",
        "      nn.GroupNorm(1, out_channels * m),\n",
        "      nn.Conv2d(out_channels * m, out_channels, 3, 1, 1)\n",
        "    )\n",
        "\n",
        "    if in_channels == out_channels:\n",
        "      self.shortcut = nn.Identity()\n",
        "    else:\n",
        "      self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "\n",
        "  def forward(self, x, t_emb=None, class_emb=None):\n",
        "    x_proj = self.shortcut(x)\n",
        "\n",
        "    if t_emb is None or self.t_fc is None:\n",
        "      t_emb = 0\n",
        "    else:\n",
        "      t_emb = rearrange(self.t_fc(F.silu(t_emb)), 'b c -> b c 1 1')\n",
        "\n",
        "    if class_emb is None or self.c_fc is None:\n",
        "      class_emb = 0\n",
        "    else:\n",
        "      class_emb = rearrange(self.c_fc(F.silu(class_emb)), 'b c -> b c 1 1')\n",
        "\n",
        "    return self.sequential(self.ds_conv(x) + t_emb + class_emb) + x_proj"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9KTuLaKOz87e"
      },
      "source": [
        "To do later: distributed diffusion w/ ImageNet\n",
        "\n",
        "```\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, max_t, m, n_class):\n",
        "    super().__init__()\n",
        "    self.max_t, self.m = max_t, m\n",
        "\n",
        "    x_size = 224  # ImageNet dimension\n",
        "\n",
        "    t_emb_d = 4 * x_size\n",
        "    t_emb = position_embeddings(max_t + 1, t_emb_d)\n",
        "    self.t_emb = nn.Parameter(t_emb, requires_grad=False)\n",
        "\n",
        "    class_emb_d = 4 * x_size\n",
        "    self.class_emb = nn.Embedding(n_class, class_emb_d)\n",
        "\n",
        "    downscales, upscales = zip(\n",
        "      *[conv_down_up(x_size, 2 ** x, 2 * 2 ** x, 3, 2, 1) for x in range(4, 9)]\n",
        "    )\n",
        "\n",
        "    self.downscales = nn.ModuleList(downscales)\n",
        "    self.upscales = nn.ModuleList(reversed(upscales))\n",
        "\n",
        "    Block = lambda in_channels, out_channels: nn.ModuleList([\n",
        "      ConvNeXtBlock(in_channels, out_channels, t_emb_d, class_emb_d, m),\n",
        "      Attention(out_channels)\n",
        "    ])\n",
        "\n",
        "    self.downblocks = nn.ModuleList([\n",
        "      Block(3, 16), *[Block(2 ** x, 2 ** x) for x in range(5, 10)]\n",
        "    ])\n",
        "    self.upblocks = nn.ModuleList([\n",
        "      Block(2 ** x, 2 ** x // 2) for x in range(9, 4, -1)\n",
        "    ])\n",
        "\n",
        "    self.outconv = ConvNeXtBlock(16, 3, t_emb_d, class_emb_d, m)\n",
        "\n",
        "  def forward(self, x, t=None, classes=None):\n",
        "    cache = []\n",
        "\n",
        "    conv, attn = self.downblocks[0]\n",
        "    t_emb = None if t is None else self.t_emb.index_select(0, t)\n",
        "    class_emb = None if classes is None else self.class_emb(classes)\n",
        "\n",
        "    x = attn(conv(x, t_emb, class_emb))\n",
        "    for down, (conv, attn) in zip(self.downscales, self.downblocks[1:]):\n",
        "      cache.append(x)\n",
        "      x = attn(conv(down(x), t_emb, class_emb))\n",
        "\n",
        "    for up, (conv, attn) in zip(self.upscales, self.upblocks):\n",
        "      x = attn(conv(torch.cat([up(x), cache.pop()], 1), t_emb, class_emb))\n",
        "\n",
        "    return self.outconv(x, t_emb, class_emb)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QowQH3JC935-"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, max_t, m, n_class):\n",
        "    super().__init__()\n",
        "    self.max_t, self.m = max_t, m\n",
        "\n",
        "    x_size = 28  # MNIST dimension\n",
        "\n",
        "    t_emb_d = 4 * x_size\n",
        "    t_emb = position_embeddings(max_t + 1, t_emb_d)\n",
        "    self.t_emb = nn.Parameter(t_emb, requires_grad=False)\n",
        "\n",
        "    class_emb_d = 4 * x_size\n",
        "    self.class_emb = nn.Embedding(n_class, class_emb_d)\n",
        "\n",
        "    downscales, upscales = zip(\n",
        "      conv_down_up(x_size, 16, 32, 3, 2, 1),\n",
        "      conv_down_up(x_size, 32, 64, 3, 2, 1),\n",
        "      conv_down_up(x_size, 64, 128, 3, 3, 1)\n",
        "    )\n",
        "\n",
        "    self.downscales = nn.ModuleList(downscales)\n",
        "    self.upscales = nn.ModuleList(reversed(upscales))\n",
        "\n",
        "    Block = lambda in_channels, out_channels: nn.ModuleList([\n",
        "      ConvNeXtBlock(in_channels, out_channels, t_emb_d, class_emb_d, m),\n",
        "      Attention(out_channels)\n",
        "    ])\n",
        "\n",
        "    self.downblocks = nn.ModuleList([\n",
        "      Block(1, 16), Block(32, 32), Block(64, 64), Block(128, 128)\n",
        "    ])\n",
        "    self.upblocks = nn.ModuleList([\n",
        "      Block(128, 64), Block(64, 32), Block(32, 8)\n",
        "    ])\n",
        "\n",
        "    self.final_conv = ConvNeXtBlock(8, 1, t_emb_d, class_emb_d, m)\n",
        "\n",
        "  def forward(self, x, t=None, classes=None):\n",
        "    cache = []\n",
        "\n",
        "    conv, attn = self.downblocks[0]\n",
        "    t_emb = None if t is None else self.t_emb.index_select(0, t)\n",
        "    class_emb = None if classes is None else self.class_emb(classes)\n",
        "\n",
        "    x = attn(conv(x, t_emb, class_emb))\n",
        "    for down, (conv, attn) in zip(self.downscales, self.downblocks[1:]):\n",
        "      cache.append(x)\n",
        "      x = attn(conv(down(x), t_emb, class_emb))\n",
        "\n",
        "    for up, (conv, attn) in zip(self.upscales, self.upblocks):\n",
        "      x = attn(conv(torch.cat([up(x), cache.pop()], 1), t_emb, class_emb))\n",
        "\n",
        "    return self.final_conv(x, t_emb, class_emb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2NVtJwyQGI"
      },
      "source": [
        "### Forward/reverse diffusion sampler\n",
        "\n",
        "A simple way to speed up diffusion inference: denoise many timesteps at a time instead of one by one. We also achieve greater performance by making the inference process deterministic, i.e., setting $\\eta = 0$, in which case our DDPM ($\\eta = 1$) becomes a DDIM. DDIM allows us achieve a good speed-quality tradeoff, and it does this by making the reverse diffusion process (the denoising process) non-Markovian.\n",
        "\n",
        "We use a cosine schedule for the noise\n",
        "\n",
        "$$\\bar\\alpha = \\frac{f(t)}{f(0)},\\; f(t) = \\cos^2\\left(\\frac{t/T + s}{1 + s}\\right)$$\n",
        "\n",
        "where $s = 0.008$ and is used as an offset to prevent $\\beta_t = 1 - \\frac{\\bar\\alpha_t}{\\bar\\alpha_{t - 1}}$ from being too small near $t = 0$. $\\beta_t$ is also clipped to be between $0$ and $0.999$ so as to prevent singularities near $t = T$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L0eQA-Ssy5fT"
      },
      "outputs": [],
      "source": [
        "def cosine_schedule(max_t):\n",
        "  t = torch.arange(0, max_t + 1)\n",
        "  return ((torch.pi / 2) * (t / max_t + 0.008) / (1 + 0.008)).cos() ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QBe9XvfB7HZ3"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "  def __init__(self, max_t, n_step, schedule, device, eta=1, shape=(1, 28, 28)):\n",
        "    self.n_step = n_step\n",
        "    self.device = device\n",
        "    self.eta = eta\n",
        "    self.shape = shape\n",
        "\n",
        "    abar = schedule(max_t).to(device)\n",
        "    self.abar = abar / abar[0]\n",
        "    self.sqrt_abar = rearrange(self.abar.sqrt(), 'l -> l 1 1 1')\n",
        "    self.sqrt_bbar = rearrange((1 - self.abar).sqrt(), 'l -> l 1 1 1')\n",
        "\n",
        "    self.tau = torch.arange(0, max_t, max_t // n_step) + 1\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    eps = torch.randn_like(x, device=self.device)\n",
        "    mu_t = self.sqrt_abar.index_select(0, t) * x\n",
        "    noise_t = self.sqrt_bbar.index_select(0, t) * eps\n",
        "    return mu_t + noise_t, eps\n",
        "\n",
        "  # DDIM reverse process loop\n",
        "  @torch.no_grad()\n",
        "  def loop(self, model, n_sample, classes=None):\n",
        "    model.eval()\n",
        "\n",
        "    if classes is not None:\n",
        "      assert n_sample == len(classes), 'n_sample must equal batch size'\n",
        "\n",
        "    x_t = torch.randn(n_sample, *self.shape, device=self.device)\n",
        "    xs = [x_t]\n",
        "    tau_b = repeat(self.tau, 'l -> l n', n=n_sample).to(self.device)\n",
        "\n",
        "    for i in range(self.n_step - 1, -1, -1):\n",
        "      bbar = 1 - self.abar[self.tau[i]]\n",
        "      beta = 1 - self.abar[self.tau[i]] / self.abar[self.tau[i - 1]]\n",
        "      beta = (beta * (1 - self.abar[self.tau[i - 1]]) / bbar).clip(0, 0.999)\n",
        "      alpha = 1 - beta\n",
        "\n",
        "      eps = model(x_t, tau_b[i], classes)\n",
        "      x_0 = alpha.rsqrt() * (x_t - beta * eps * bbar.rsqrt())\n",
        "\n",
        "      if i == 0:\n",
        "        return xs + [x_0]\n",
        "\n",
        "      noise_t = beta.sqrt() * torch.randn_like(x_t, device=self.device)\n",
        "      x_t = x_0 if self.eta == 0 else x_0 + self.eta * noise_t\n",
        "      xs.append(x_0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8yMcBL0EyOF4"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDi5lHp-CT5h",
        "outputId": "4153d9d0-c09b-40f9-845e-d85fd3c0e21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 85396496.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 59264038.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24220648.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3811630.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = MNIST(\n",
        "  root='./',\n",
        "  download=True,\n",
        "  transform=transforms.Compose([\n",
        "    transforms.ToTensor(), transforms.Lambda(lambda x: 2 * x - 1)\n",
        "  ])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mvfyNFEP8EOs"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "# weight_decay = 1e-2  # if using AdamW\n",
        "\n",
        "max_t = 500\n",
        "n_step = 35  # number of steps in reverse process\n",
        "device = 'cuda'\n",
        "eta = 0.1  # eta = 0 is DDIM, eta = 1 is DDPM\n",
        "\n",
        "sampler = Sampler(max_t, n_step, cosine_schedule, device, eta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6WQsRXnoLw0n"
      },
      "outputs": [],
      "source": [
        "model = UNet(max_t, m=3, n_class=10).to(device).train()\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "dataloader = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e08UrDWE6RJ-",
        "outputId": "baeffa7a-461a-4a9a-e3a4-290ae7d0394c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 468/468 [00:39<00:00, 11.88it/s, loss=0.0802]\n",
            "100%|==========| 468/468 [00:35<00:00, 13.34it/s, loss=0.0742]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.55it/s, loss=0.0515]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.84it/s, loss=0.0570]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.54it/s, loss=0.0442]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.75it/s, loss=0.0438]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.87it/s, loss=0.0468]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.77it/s, loss=0.0447]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.77it/s, loss=0.0477]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.70it/s, loss=0.0417]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.90it/s, loss=0.0457]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.48it/s, loss=0.0438]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.73it/s, loss=0.0450]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.69it/s, loss=0.0397]\n",
            "100%|==========| 468/468 [00:33<00:00, 14.01it/s, loss=0.0396]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.80it/s, loss=0.0392]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.93it/s, loss=0.0406]\n",
            "100%|==========| 468/468 [00:33<00:00, 14.05it/s, loss=0.0421]\n",
            "100%|==========| 468/468 [00:34<00:00, 13.69it/s, loss=0.0378]\n",
            "100%|==========| 468/468 [00:33<00:00, 13.88it/s, loss=0.0380]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  bar = tqdm(dataloader, ascii=' >=')\n",
        "  for x, classes in bar:\n",
        "    t = torch.randint(1, max_t + 1, (batch_size,), device=device)\n",
        "    x_t, eps = sampler.forward(x.to(device), t)\n",
        "    loss = F.mse_loss(model(x_t, t, classes.to(device)), eps)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "16xSoIaLyHOW"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PUU8Bum2Ud8"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), '...')\n",
        "# _ = model.load_state_dict(torch.load('...'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rLq0gny1YKkT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2 ** 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk3zWotyZkhA"
      },
      "outputs": [],
      "source": [
        "n_sample = 81\n",
        "classes = torch.randint(0, 10, (n_sample,), device=device)\n",
        "# or classes = torch.tensor([class_1, ..., class_{n_sample}], device=device)\n",
        "xs = sampler.loop(model, n_sample, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vjobxG7gfhQC",
        "outputId": "ad6ec28f-9b1b-4889-dc5a-47d8e64e855e"
      },
      "outputs": [],
      "source": [
        "t = -1  # timestep to display\n",
        "rows = 9  # number of rows to display n_sample samples\n",
        "\n",
        "batch = [x for x in rearrange(xs[t], 'b 1 h w -> b h w').cpu()]\n",
        "classes = classes.cpu().tolist()\n",
        "\n",
        "assert n_sample % rows == 0\n",
        "cols = n_sample // rows\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.2, rows * 1.2))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f'{classes[i]}')\n",
        "  ax.imshow(batch[i], cmap='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AKPfubgyyL28"
      },
      "source": [
        "### Animate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gn2phJ3kdCb_",
        "outputId": "2d9c75da-0bf2-43c2-c998-bc54aede3822"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "skip = 1  # skip these many timesteps to display\n",
        "pad = transforms.Pad(2)\n",
        "reshape = lambda x: pad(rearrange(pad(x), '(r s) 1 h w -> (r h) (s w)', r=rows))\n",
        "\n",
        "start = max_t % skip\n",
        "for x in ([xs[0]] if start else []) + xs[start::skip]:\n",
        "  images.append(reshape(x).cpu().numpy())\n",
        "\n",
        "\n",
        "def update(i):\n",
        "  plt.clf()\n",
        "  plt.imshow(images[i], cmap='plasma')\n",
        "  plt.axis('off')\n",
        "\n",
        "\n",
        "plt.ioff()\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ani = animation.FuncAnimation(\n",
        "  fig, update, frames=range(len(images)), interval=200\n",
        ")\n",
        "\n",
        "display(HTML(ani.to_jshtml()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teeluvMfYMF5"
      },
      "outputs": [],
      "source": [
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
